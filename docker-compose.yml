services:
  triton:
    build: .
    restart: always
    ports:
      - "8000:8000"  # HTTP port
      - "8001:8001"  # gRPC port
      - "8002:8002"  # Metrics port
    volumes:
      - ./models:/models
      - ./model_cache:/model_cache
    environment:
      - CUDA_VISIBLE_DEVICES=0  # Set which GPU to use, if any
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    command: [
      "tritonserver",
      "--model-repository=/models",
      "--log-verbose=1",
      "--log-info=1",
      "--log-warning=1",
      "--log-error=1"
    ]
